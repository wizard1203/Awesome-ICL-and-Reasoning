
@article{li2024language,
  title={What Do Language Models Learn in Context? The Structured Task Hypothesis},
  author={Li, Jiaoda and Hou, Yifan and Sachan, Mrinmaya and Cotterell, Ryan},
  journal={arXiv preprint arXiv:2406.04216},
  year={2024}
}

Here are the two entries transformed into .bib format:

```bibtex
@article{akyürek2024incontext,
  author    = {Ekin Akyürek and Bailin Wang and Yoon Kim and Jacob Andreas},
  title     = {In-context language learning: Architectures and algorithms},
  year      = {2024},
  journal   = {arxiv},
  volume    = {arXiv:2401.12973},
}

@inproceedings{NEURIPS2020_1457c0d6,
  author    = {Tom Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared D Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel Ziegler and Jeffrey Wu and Clemens Winter and Chris Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
  title     = {Language models are few-shot learners},
  year      = {2020},
  booktitle = {Advances in Neural Information Processing Systems},
  volume    = {33},
  pages     = {1877--1901},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf}
}
```

You can add these entries to your .bib file for use in your LaTeX document or bibliography management software.


@article{DBLP:journals/corr/abs-2107-03374,
  author    = {Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Pond{\'{e}} de Oliveira Pinto and Jared Kaplan and Harrison Edwards and Yuri Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and Dave Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert{-}Voss and William Hebgen Guss and Alex Nichol and Alex Paino and Nikolas Tezak and Jie Tang and Igor Babuschkin and Suchir Balaji and Shantanu Jain and William Saunders and Christopher Hesse and Andrew N. Carr and Jan Leike and Joshua Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},
  title     = {Evaluating large language models trained on code},
  year      = {2021},
  journal   = {Computing Research Repository},
  volume    = {arXiv:2107.03374},
  url       = {https://arxiv.org/abs/2107.03374}
}

@inproceedings{dai-etal-2023-gpt,
  author    = {Damai Dai and Yutao Sun and Li Dong and Yaru Hao and Shuming Ma and Zhifang Sui and Furu Wei},
  title     = {Why can {GPT} learn in-context? language models secretly perform gradient descent as meta-optimizers},
  year      = {2023},
  booktitle = {Findings of the Association for Computational Linguistics: ACL 2023},
  pages     = {4005--4019},
  address   = {Toronto, Canada},
  publisher = {Association for Computational Linguistics},
  url       = {https://doi.org/10.18653/v1/2023.findings-acl.247}
}

@inproceedings{pmlr-v70-finn17a,
  author    = {Chelsea Finn and Pieter Abbeel and Sergey Levine},
  title     = {Model-agnostic meta-learning for fast adaptation of deep networks},
  year      = {2017},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  volume    = {70},
  pages     = {1126--1135},
  publisher = {PMLR},
  url       = {https://proceedings.mlr.press/v70/finn17a.html}
}

@inproceedings{garg2022what,
  author    = {Shivam Garg and Dimitris Tsipras and Percy S Liang and Gregory Valiant},
  title     = {What can transformers learn in-context? {A} case study of simple function classes},
  year      = {2022},
  booktitle = {Advances in Neural Information Processing Systems},
  volume    = {35},
  pages     = {30583--30598}
}

@article{hahn2023theory,
  author    = {Michael Hahn and Navin Goyal},
  title     = {A theory of emergent in-context learning as implicit structure induction},
  year      = {2023},
  journal   = {Computing Research Repository},
  volume    = {arXiv:2303.07971},
  version   = {1},
  url       = {https://arxiv.org/abs/2303.07971}
}

@inproceedings{DBLP:conf/kdd/HuL04,
  author    = {Minqing Hu and Bing Liu},
  title     = {Mining and summarizing customer reviews},
  year      = {2004},
  booktitle = {Proceedings of the Tenth {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining},
  pages     = {168--177},
  address   = {Seattle, Washington, USA},
  publisher = {ACM}
}

@article{kasneci2023chatgpt,
  author    = {Enkelejda Kasneci and Kathrin Se{\ss}ler and Stefan K{\"u}chemann and Maria Bannert and Daryna Dementieva and Frank Fischer and Urs Gasser and Georg Groh and Stephan G{\"u}nnemann and Eyke H{\"u}llermeier and et~al.},
  title     = {{ChatGPT} for good? {On} opportunities and challenges of large language models for education},
  year      = {2023},
  journal   = {Learning and individual differences},
  volume    = {103},
  pages     = {102274},
  url       = {https://www.sciencedirect.com/science/article/pii/S1041608023000195}
}

@inproceedings{kossen2024incontext,
  author    = {Jannik Kossen and Yarin Gal and Tom Rainforth},
  title     = {In-context learning learns label relationships but is not conventional learning},
  year      = {2024},
  booktitle = {International Conference on Learning Representations}
}

@inproceedings{miller-1994-wordnet,
  author    = {George A. Miller},
  title     = {{W}ord{N}et: A lexical database for {E}nglish},
  year      = {1994},
  booktitle = {{H}uman {L}anguage {T}echnology: Proceedings of a Workshop held at {P}lainsboro, {N}ew {J}ersey, {M}arch 8-11, 1994}
}

@inproceedings{min-etal-2022-rethinking,
  author    = {Sewon Min and Xinxi Lyu and Ari Holtzman and Mikel Artetxe and Mike Lewis and Hannaneh Hajishirzi and Luke Zettlemoyer},
  title     = {Rethinking the role of demonstrations: What makes in-context learning work?},
  year      = {2022},
  booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages     = {11048--11064},
  address   = {Abu Dhabi, United Arab Emirates},
  publisher = {Association for Computational Linguistics},
  url       = {https://doi.org/10.18653/v1/2022.emnlp-main.759}
}

@article{olsson2022incontext,
  author    = {Catherine Olsson and Nelson Elhage and Neel Nanda and Nicholas Joseph and Nova DasSarma and Tom Henighan and Ben Mann and Amanda Askell and Yuntao Bai and Anna Chen and Tom Conerly and Dawn Drain and Deep Ganguli and Zac Hatfield-Dodds and Danny Hernandez and Scott Johnston and Andy Jones and Jackson Kernion and Liane Lovitt and Kamal Ndousse and Dario Amodei and Tom Brown and Jack Clark and Jared Kaplan and Sam McCandlish and Chris Olah},
  title     = {In-context learning and induction heads},
  year      = {2022},
  journal   = {Computing Research Repository},
  volume    = {arXiv:2209.11895}
}

@article{DBLP:journals/corr/abs-2303-08774,
  author    = {OpenAI},
  title     = {{GPT-4} technical report},
  year      = {2023},
  journal   = {Computing Research Repository},
  volume    = {arXiv:2303.08774},
  url       = {https://doi.org/10.48550/arXiv.2303.08774}
}

@inproceedings{pan-etal-2023-context,
  author    = {Jane Pan and Tianyu Gao and Howard Chen and Danqi Chen},
  title     = {What in-context learning {``}learns{''} in-context: Disentangling task recognition and task learning},
  year      = {2023},
  booktitle = {Findings of the Association for Computational Linguistics: ACL 2023},
  pages     = {8298--8319},
  address   = {Toronto, Canada},
  publisher = {Association for Computational Linguistics},
  url       = {https://doi.org/10.18653/v1/2023.findings-acl.527}
}

@article{radford2019language,
  author    = {Alec Radford and Jeffrey Wu and Rewon Child and David Luan and Dario Amodei and Ilya Sutskever},
  title     = {Language models are unsupervised multitask learners},
  year      = {2019},
  howpublished = {Github Repository},
  url       = {https://github.com/openai/gpt-2}
}

@inproceedings{raventos2023pretraining,
  author    = {Allan Raventos and Mansheej Paul and Feng Chen and Surya Ganguli},
  title     = {Pretraining task diversity and the emergence of non-bayesian in-context learning for regression},
  year      = {2023},
  booktitle = {Thirty-seventh Conference on Neural Information Processing Systems}
}

@article{schmidhuber87,
  author    = {Jürgen Schmidhuber},
  title     = {\emph{Evolutionary principles in self-referential learning, or on learning how to learn: The meta-meta-... hook}},
  journal   = {Ph.D. thesis},
  year      = {1987},
}

@article{shen2024pretrained,
  author    = {Lingfeng Shen and Aayush Mishra and Daniel Khashabi},
  title     = {Do pretrained transformers learn in-context by gradient descent?},
  year      = {2024},
  journal   = {Computing Research Repository},
  volume    = {arXiv:2310.08540},
  version   = {5},
  url       = {https://arxiv.org/abs/2310.08540}
}

@inproceedings{singh2023transient,
  author    = {Aaditya K Singh and Stephanie C.Y. Chan and Ted Moskovitz and Erin Grant and Andrew M Saxe and Felix Hill},
  title     = {The transient nature of emergent in-context learning in transformers},
  year      = {2023},
  booktitle   = {Thirty-seventh Conference on Neural Information Processing Systems},
}

@inproceedings{socher-etal-2013-recursive,
  author    = {Richard Socher and Alex Perelygin and Jean Wu and Jason Chuang and Christopher D. Manning and Andrew Ng and Christopher Potts},
  title     = {Recursive deep models for semantic compositionality over a sentiment treebank},
  year      = {2013},
  booktitle = {Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing},
  pages     = {1631--1642},
  address   = {Seattle, Washington, USA},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/D13-1170}
}

@article{thirunavukarasu2023large,
  author    = {Arun James Thirunavukarasu and Darren Shu Jeng Ting and Kabilan Elangovan and Laura Gutierrez and Ting Fang Tan and Daniel Shu Wei Ting},
  title     = {Large language models in medicine},
  year      = {2023},
  journal   = {Nature medicine},
  volume    = {29},
  number    = {8},
  pages     = {1930--1940},
  url       = {https://www.nature.com/articles/s41591-023-02448-8}
}

@article{Thrun1998,
  author    = {Sebastian Thrun and Lorien Pratt},
  title     = {\emph{Learning to Learn: Introduction and Overview}},
  year      = {1998},
  pages     = {3--17},
  publisher = {Springer US},
  address   = {Boston, MA},
  url       = {https://doi.org/10.1007/978-1-4615-5529-2_1}
}

@inproceedings{touvron2023llama,
  author    = {Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
  title     = {Llama 2: Open foundation and fine-tuned chat models},
  year      = {2023},
  journal   = {arXiv},
  volume    = {arXiv:2307.09288},
  version   = {2},
}

@article{vilalta01,
  author    = {Ricardo Vilalta and Youssef Drissi},
  title     = {A perspective view and survey of meta-learning},
  year      = {2001},
  journal   = {Artificial Intelligence Review},
  volume    = {18}
}

@inproceedings{pmlr-v202-von-oswald23a,
  author    = {Johannes Von Oswald and Eyvind Niklasson and Ettore Randazzo and Joao Sacramento and Alexander Mordvintsev and Andrey Zhmoginov and Max Vladymyrov},
  title     = {Transformers learn in-context by gradient descent},
  year      = {2023},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  volume    = {202},
  pages     = {35151--35174},
  publisher = {PMLR},
  url       = {https://proceedings.mlr.press/v202/von-oswald23a.html}
}

@inproceedings{wang2023large,
  author    = {Xinyi Wang and Wanrong Zhu and Michael Saxon and Mark Steyvers and William Yang Wang},
  title     = {Large language models are implicitly topic models: Explaining and finding good demonstrations for in-context learning},
  year      = {2023},
  booktitle = {Workshop on Efficient Systems for Foundation Models @ ICML2023}
}

@article{wei2022emergent,
  author    = {Jason Wei and Yi Tay and Rishi Bommasani and Colin Raffel and Barret Zoph and Sebastian Borgeaud and Dani Yogatama and Maarten Bosma and Denny Zhou and Donald Metzler and Ed H. Chi and Tatsunori Hashimoto and Oriol Vinyals and Percy Liang and Jeff Dean and William Fedus},
  title     = {Emergent abilities of large language models},
  year      = {2022},
  journal   = {Transactions on Machine Learning Research},
  note      = {Survey Certification}
}

@article{wei2023larger,
  author    = {Jerry Wei and Jason Wei and Yi Tay and Dustin Tran and Albert Webson and Yifeng Lu and Xinyun Chen and Hanxiao Liu and Da Huang and Denny Zhou and Tengyu Ma},
  title     = {Larger language models do in-context learning differently},
  year      = {2023},
  journal   = {Computing Research Repository},
  volume    = {arXiv:2303.03846},
  version   = {2},
  url       = {https://arxiv.org/abs/2303.03846}
}

@inproceedings{wies2023learnability,
  author    = {Noam Wies and Yoav Levine and Amnon Shashua},
  title     = {The learnability of in-context learning},
  year      = {2023},
  booktitle = {Thirty-seventh Conference on Neural Information Processing Systems}
}

@inproceedings{wolf-etal-2020-transformers,
  author    = {Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush},
  title     = {Transformers: State-of-the-art natural language processing},
  year      = {2020},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  pages     = {38--45},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://www.aclweb.org/anthology/2020.emnlp-demos.6}
}

@inproceedings{xie2022an,
  author    = {Sang Michael Xie and Aditi Raghunathan and Percy Liang and Tengyu Ma},
  title     = {An explanation of in-context learning as implicit {B}ayesian inference},
  year      = {2022},
  booktitle = {International Conference on Learning Representations}
}

@inproceedings{DBLP:conf/nips/ZhangZL15,
  author    = {Xiang Zhang and Junbo Jake Zhao and Yann LeCun},
  title     = {Character-level convolutional networks for text classification},
  year      = {2015},
  booktitle = {Advances in Neural Information Processing Systems 28},
  pages     = {649--657},
  address   = {Montreal, Quebec, Canada}
}













