
@article{li2024language,
  title={What Do Language Models Learn in Context? The Structured Task Hypothesis},
  author={Li, Jiaoda and Hou, Yifan and Sachan, Mrinmaya and Cotterell, Ryan},
  journal={arXiv preprint arXiv:2406.04216},
  year={2024}
}

@article{akyürek2024incontext,
  author    = {Ekin Akyürek and Bailin Wang and Yoon Kim and Jacob Andreas},
  title     = {In-context language learning: Architectures and algorithms},
  year      = {2024},
  journal   = {arxiv},
  volume    = {arXiv:2401.12973},
}

@inproceedings{NEURIPS2020_1457c0d6,
  author    = {Tom Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared D Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel Ziegler and Jeffrey Wu and Clemens Winter and Chris Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
  title     = {Language models are few-shot learners},
  year      = {2020},
  booktitle = {Advances in Neural Information Processing Systems},
  volume    = {33},
  pages     = {1877--1901},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf}
}



@article{DBLP:journals/corr/abs-2107-03374,
  author    = {Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Pond{\'{e}} de Oliveira Pinto and Jared Kaplan and Harrison Edwards and Yuri Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and Dave Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert{-}Voss and William Hebgen Guss and Alex Nichol and Alex Paino and Nikolas Tezak and Jie Tang and Igor Babuschkin and Suchir Balaji and Shantanu Jain and William Saunders and Christopher Hesse and Andrew N. Carr and Jan Leike and Joshua Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},
  title     = {Evaluating large language models trained on code},
  year      = {2021},
  journal   = {arxiv},
  volume    = {arXiv:2107.03374},
  url       = {https://arxiv.org/abs/2107.03374}
}

@inproceedings{dai-etal-2023-gpt,
  author    = {Damai Dai and Yutao Sun and Li Dong and Yaru Hao and Shuming Ma and Zhifang Sui and Furu Wei},
  title     = {Why can {GPT} learn in-context? language models secretly perform gradient descent as meta-optimizers},
  year      = {2023},
  booktitle = {Findings of the Association for Computational Linguistics: ACL 2023},
  pages     = {4005--4019},
  address   = {Toronto, Canada},
  publisher = {Association for Computational Linguistics},
  url       = {https://doi.org/10.18653/v1/2023.findings-acl.247}
}

@inproceedings{pmlr-v70-finn17a,
  author    = {Chelsea Finn and Pieter Abbeel and Sergey Levine},
  title     = {Model-agnostic meta-learning for fast adaptation of deep networks},
  year      = {2017},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  volume    = {70},
  pages     = {1126--1135},
  publisher = {PMLR},
  url       = {https://proceedings.mlr.press/v70/finn17a.html}
}

@inproceedings{garg2022what,
  author    = {Shivam Garg and Dimitris Tsipras and Percy S Liang and Gregory Valiant},
  title     = {What can transformers learn in-context? {A} case study of simple function classes},
  year      = {2022},
  booktitle = {Advances in Neural Information Processing Systems},
  volume    = {35},
  pages     = {30583--30598}
}

@article{hahn2023theory,
  author    = {Michael Hahn and Navin Goyal},
  title     = {A theory of emergent in-context learning as implicit structure induction},
  year      = {2023},
  journal   = {arxiv},
  volume    = {arXiv:2303.07971},
  version   = {1},
  url       = {https://arxiv.org/abs/2303.07971}
}

@inproceedings{DBLP:conf/kdd/HuL04,
  author    = {Minqing Hu and Bing Liu},
  title     = {Mining and summarizing customer reviews},
  year      = {2004},
  booktitle = {Proceedings of the Tenth {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining},
  pages     = {168--177},
  address   = {Seattle, Washington, USA},
  publisher = {ACM}
}

@article{kasneci2023chatgpt,
  author    = {Enkelejda Kasneci and Kathrin Se{\ss}ler and Stefan K{\"u}chemann and Maria Bannert and Daryna Dementieva and Frank Fischer and Urs Gasser and Georg Groh and Stephan G{\"u}nnemann and Eyke H{\"u}llermeier and et~al.},
  title     = {{ChatGPT} for good? {On} opportunities and challenges of large language models for education},
  year      = {2023},
  journal   = {Learning and individual differences},
  volume    = {103},
  pages     = {102274},
  url       = {https://www.sciencedirect.com/science/article/pii/S1041608023000195}
}

@inproceedings{kossen2024incontext,
  author    = {Jannik Kossen and Yarin Gal and Tom Rainforth},
  title     = {In-context learning learns label relationships but is not conventional learning},
  year      = {2024},
  booktitle = {International Conference on Learning Representations}
}

@inproceedings{miller-1994-wordnet,
  author    = {George A. Miller},
  title     = {{W}ord{N}et: A lexical database for {E}nglish},
  year      = {1994},
  booktitle = {{H}uman {L}anguage {T}echnology: Proceedings of a Workshop held at {P}lainsboro, {N}ew {J}ersey, {M}arch 8-11, 1994}
}

@inproceedings{min-etal-2022-rethinking,
  author    = {Sewon Min and Xinxi Lyu and Ari Holtzman and Mikel Artetxe and Mike Lewis and Hannaneh Hajishirzi and Luke Zettlemoyer},
  title     = {Rethinking the role of demonstrations: What makes in-context learning work?},
  year      = {2022},
  booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages     = {11048--11064},
  address   = {Abu Dhabi, United Arab Emirates},
  publisher = {Association for Computational Linguistics},
  url       = {https://doi.org/10.18653/v1/2022.emnlp-main.759}
}

@article{olsson2022incontext,
  author    = {Catherine Olsson and Nelson Elhage and Neel Nanda and Nicholas Joseph and Nova DasSarma and Tom Henighan and Ben Mann and Amanda Askell and Yuntao Bai and Anna Chen and Tom Conerly and Dawn Drain and Deep Ganguli and Zac Hatfield-Dodds and Danny Hernandez and Scott Johnston and Andy Jones and Jackson Kernion and Liane Lovitt and Kamal Ndousse and Dario Amodei and Tom Brown and Jack Clark and Jared Kaplan and Sam McCandlish and Chris Olah},
  title     = {In-context learning and induction heads},
  year      = {2022},
  journal   = {arxiv},
  volume    = {arXiv:2209.11895}
}

@article{DBLP:journals/corr/abs-2303-08774,
  author    = {OpenAI},
  title     = {{GPT-4} technical report},
  year      = {2023},
  journal   = {arxiv},
  volume    = {arXiv:2303.08774},
  url       = {https://doi.org/10.48550/arXiv.2303.08774}
}

@inproceedings{pan-etal-2023-context,
  author    = {Jane Pan and Tianyu Gao and Howard Chen and Danqi Chen},
  title     = {What in-context learning {``}learns{''} in-context: Disentangling task recognition and task learning},
  year      = {2023},
  booktitle = {Findings of the Association for Computational Linguistics: ACL 2023},
  pages     = {8298--8319},
  address   = {Toronto, Canada},
  publisher = {Association for Computational Linguistics},
  url       = {https://doi.org/10.18653/v1/2023.findings-acl.527}
}

@article{radford2019language,
  author    = {Alec Radford and Jeffrey Wu and Rewon Child and David Luan and Dario Amodei and Ilya Sutskever},
  title     = {Language models are unsupervised multitask learners},
  year      = {2019},
  howpublished = {Github Repository},
  url       = {https://github.com/openai/gpt-2}
}

@inproceedings{raventos2023pretraining,
  author    = {Allan Raventos and Mansheej Paul and Feng Chen and Surya Ganguli},
  title     = {Pretraining task diversity and the emergence of non-bayesian in-context learning for regression},
  year      = {2023},
  booktitle = {Thirty-seventh Conference on Neural Information Processing Systems}
}

@article{schmidhuber87,
  author    = {Jürgen Schmidhuber},
  title     = {\emph{Evolutionary principles in self-referential learning, or on learning how to learn: The meta-meta-... hook}},
  journal   = {Ph.D. thesis},
  year      = {1987},
}

@article{shen2024pretrained,
  author    = {Lingfeng Shen and Aayush Mishra and Daniel Khashabi},
  title     = {Do pretrained transformers learn in-context by gradient descent?},
  year      = {2024},
  journal   = {arxiv},
  volume    = {arXiv:2310.08540},
  version   = {5},
  url       = {https://arxiv.org/abs/2310.08540}
}

@inproceedings{singh2023transient,
  author    = {Aaditya K Singh and Stephanie C.Y. Chan and Ted Moskovitz and Erin Grant and Andrew M Saxe and Felix Hill},
  title     = {The transient nature of emergent in-context learning in transformers},
  year      = {2023},
  booktitle   = {Thirty-seventh Conference on Neural Information Processing Systems},
}

@inproceedings{socher-etal-2013-recursive,
  author    = {Richard Socher and Alex Perelygin and Jean Wu and Jason Chuang and Christopher D. Manning and Andrew Ng and Christopher Potts},
  title     = {Recursive deep models for semantic compositionality over a sentiment treebank},
  year      = {2013},
  booktitle = {Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing},
  pages     = {1631--1642},
  address   = {Seattle, Washington, USA},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/D13-1170}
}

@article{thirunavukarasu2023large,
  author    = {Arun James Thirunavukarasu and Darren Shu Jeng Ting and Kabilan Elangovan and Laura Gutierrez and Ting Fang Tan and Daniel Shu Wei Ting},
  title     = {Large language models in medicine},
  year      = {2023},
  journal   = {Nature medicine},
  volume    = {29},
  number    = {8},
  pages     = {1930--1940},
  url       = {https://www.nature.com/articles/s41591-023-02448-8}
}

@article{Thrun1998,
  author    = {Sebastian Thrun and Lorien Pratt},
  title     = {\emph{Learning to Learn: Introduction and Overview}},
  year      = {1998},
  pages     = {3--17},
  publisher = {Springer US},
  address   = {Boston, MA},
  url       = {https://doi.org/10.1007/978-1-4615-5529-2_1}
}

@inproceedings{touvron2023llama,
  author    = {Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
  title     = {Llama 2: Open foundation and fine-tuned chat models},
  year      = {2023},
  journal   = {arXiv},
  volume    = {arXiv:2307.09288},
  version   = {2},
}

@article{vilalta01,
  author    = {Ricardo Vilalta and Youssef Drissi},
  title     = {A perspective view and survey of meta-learning},
  year      = {2001},
  journal   = {Artificial Intelligence Review},
  volume    = {18}
}

@inproceedings{pmlr-v202-von-oswald23a,
  author    = {Johannes Von Oswald and Eyvind Niklasson and Ettore Randazzo and Joao Sacramento and Alexander Mordvintsev and Andrey Zhmoginov and Max Vladymyrov},
  title     = {Transformers learn in-context by gradient descent},
  year      = {2023},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  volume    = {202},
  pages     = {35151--35174},
  publisher = {PMLR},
  url       = {https://proceedings.mlr.press/v202/von-oswald23a.html}
}

@inproceedings{wang2023large,
  author    = {Xinyi Wang and Wanrong Zhu and Michael Saxon and Mark Steyvers and William Yang Wang},
  title     = {Large language models are implicitly topic models: Explaining and finding good demonstrations for in-context learning},
  year      = {2023},
  booktitle = {Workshop on Efficient Systems for Foundation Models @ ICML2023}
}

@article{wei2022emergent,
  author    = {Jason Wei and Yi Tay and Rishi Bommasani and Colin Raffel and Barret Zoph and Sebastian Borgeaud and Dani Yogatama and Maarten Bosma and Denny Zhou and Donald Metzler and Ed H. Chi and Tatsunori Hashimoto and Oriol Vinyals and Percy Liang and Jeff Dean and William Fedus},
  title     = {Emergent abilities of large language models},
  year      = {2022},
  journal   = {Transactions on Machine Learning Research},
  note      = {Survey Certification}
}

@article{wei2023larger,
  author    = {Jerry Wei and Jason Wei and Yi Tay and Dustin Tran and Albert Webson and Yifeng Lu and Xinyun Chen and Hanxiao Liu and Da Huang and Denny Zhou and Tengyu Ma},
  title     = {Larger language models do in-context learning differently},
  year      = {2023},
  journal   = {arxiv},
  volume    = {arXiv:2303.03846},
  version   = {2},
  url       = {https://arxiv.org/abs/2303.03846}
}

@inproceedings{wies2023learnability,
  author    = {Noam Wies and Yoav Levine and Amnon Shashua},
  title     = {The learnability of in-context learning},
  year      = {2023},
  booktitle = {Thirty-seventh Conference on Neural Information Processing Systems}
}

@inproceedings{wolf-etal-2020-transformers,
  author    = {Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush},
  title     = {Transformers: State-of-the-art natural language processing},
  year      = {2020},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  pages     = {38--45},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://www.aclweb.org/anthology/2020.emnlp-demos.6}
}

@inproceedings{xie2022an,
  author    = {Sang Michael Xie and Aditi Raghunathan and Percy Liang and Tengyu Ma},
  title     = {An explanation of in-context learning as implicit {B}ayesian inference},
  year      = {2022},
  booktitle = {International Conference on Learning Representations}
}

@inproceedings{DBLP:conf/nips/ZhangZL15,
  author    = {Xiang Zhang and Junbo Jake Zhao and Yann LeCun},
  title     = {Character-level convolutional networks for text classification},
  year      = {2015},
  booktitle = {Advances in Neural Information Processing Systems 28},
  pages     = {649--657},
  address   = {Montreal, Quebec, Canada}
}







#  Reasoning Single Multi calls
@article{CoT,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}


@inproceedings{Decoding,
    title={Machine Translation Decoding beyond Beam Search},
    author={Rémi Leblond and Jean-Baptiste Alayrac and L. Sifre and Miruna Pislar and Jean-Baptiste Lespiau and Ioannis Antonoglou and K. Simonyan and O. Vinyals},
    year={2021},
    url={https://www.semanticscholar.org/paper/e8cc5b6204970a88cd1b2df491aa10c4333e083e},
    booktitle={Conference on Empirical Methods in Natural Language Processing},
}



@inproceedings{chakraborty2024transfer,
title={Transfer Q-star : Principled Decoding for {LLM} Alignment},
author={Souradip Chakraborty and Soumya Suvra Ghosal and Ming Yin and Dinesh Manocha and Mengdi Wang and Amrit Bedi and Furong Huang},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=5PrShrKxoX}
}



@inproceedings{fubreak,
  title={Break the Sequential Dependency of LLM Inference Using Lookahead Decoding},
  author={Fu, Yichao and Bailis, Peter and Stoica, Ion and Zhang, Hao},
  booktitle={Forty-first International Conference on Machine Learning}
}

@article{brown2024large,
  title={Large language monkeys: Scaling inference compute with repeated sampling},
  author={Brown, Bradley and Juravsky, Jordan and Ehrlich, Ryan and Clark, Ronald and Le, Quoc V and R{\'e}, Christopher and Mirhoseini, Azalia},
  journal={arXiv preprint arXiv:2407.21787},
  year={2024}
}

@inproceedings{wangself,
  title={Self-Consistency Improves Chain of Thought Reasoning in Language Models},
  author={Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc V and Chi, Ed H and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  booktitle={The Eleventh International Conference on Learning Representations}
}

@article{ToT,
  title={Tree of thoughts: Deliberate problem solving with large language models},
  author={Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{GoT,
  title={Graph of thoughts: Solving elaborate problems with large language models},
  author={Besta, Maciej and Blach, Nils and Kubicek, Ales and Gerstenberger, Robert and Podstawski, Michal and Gianinazzi, Lukas and Gajda, Joanna and Lehmann, Tomasz and Niewiadomski, Hubert and Nyczyk, Piotr and others},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={16},
  pages={17682--17690},
  year={2024}
}

@inproceedings{luoreasoning,
  title={Reasoning on Graphs: Faithful and Interpretable Large Language Model Reasoning},
  author={LUO, LINHAO and Li, Yuan-Fang and Haf, Reza and Pan, Shirui},
  booktitle={The Twelfth International Conference on Learning Representations}
}


@inproceedings{sunthink,
  title={Think-on-Graph: Deep and Responsible Reasoning of Large Language Model on Knowledge Graph},
  author={Sun, Jiashuo and Xu, Chengjin and Tang, Lumingyuan and Wang, Saizhuo and Lin, Chen and Gong, Yeyun and Ni, Lionel and Shum, Heung-Yeung and Guo, Jian},
  booktitle={The Twelfth International Conference on Learning Representations}
}


@inproceedings{jiang2023structgpt,
  title={StructGPT: A General Framework for Large Language Model to Reason over Structured Data},
  author={Jiang, Jinhao and Zhou, Kun and Dong, Zican and Ye, Keming and Zhao, Wayne Xin and Wen, Ji-Rong},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={9237--9251},
  year={2023}
}

@article{li2023camel,
  title={Camel: Communicative agents for" mind" exploration of large language model society},
  author={Li, Guohao and Hammoud, Hasan and Itani, Hani and Khizbullin, Dmitrii and Ghanem, Bernard},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={51991--52008},
  year={2023}
}

@inproceedings{hong2024metagpt,
title={Meta{GPT}: Meta Programming for A Multi-Agent Collaborative Framework},
author={Sirui Hong and Mingchen Zhuge and Jonathan Chen and Xiawu Zheng and Yuheng Cheng and Jinlin Wang and Ceyao Zhang and Zili Wang and Steven Ka Shing Yau and Zijuan Lin and Liyang Zhou and Chenyu Ran and Lingfeng Xiao and Chenglin Wu and J{\"u}rgen Schmidhuber},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=VtmBAGCN7o}
}


@inproceedings{wu2024autogen,
  title={AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation},
  author={Wu, Qingyun and Bansal, Gagan and Zhang, Jieyu and Wu, Yiran and Li, Beibin and Zhu, Erkang and Jiang, Li and Zhang, Xiaoyun and Zhang, Shaokun and Liu, Jiale and others},
  booktitle={ICLR 2024 Workshop on Large Language Model (LLM) Agents}
}


@inproceedings{zhoulanguage,
  title={Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models},
  author={Zhou, Andy and Yan, Kai and Shlapentokh-Rothman, Michal and Wang, Haohan and Wang, Yu-Xiong},
  booktitle={Forty-first International Conference on Machine Learning}
}


@article{wangvoyager,
  title={Voyager: An Open-Ended Embodied Agent with Large Language Models},
  author={Wang, Guanzhi and Xie, Yuqi and Jiang, Yunfan and Mandlekar, Ajay and Xiao, Chaowei and Zhu, Yuke and Fan, Linxi and Anandkumar, Anima},
  journal={Transactions on Machine Learning Research}
}


@article{RecursiveReasoning,
    title={Can Transformers Learn to Solve Problems Recursively?},
    author={Shizhuo Zhang and Curt Tigges and Stella Biderman and M. Raginsky and T. Ringer},
    year={2023},
    url={https://www.semanticscholar.org/paper/45c196d28d16b2a8c0a078e8b79bcb39887a8a9f},
    journal={arXiv.org},
}


@inproceedings{duimproving,
  title={Improving Factuality and Reasoning in Language Models through Multiagent Debate},
  author={Du, Yilun and Li, Shuang and Torralba, Antonio and Tenenbaum, Joshua B and Mordatch, Igor},
  booktitle={Forty-first International Conference on Machine Learning}
}
@article{liang2023encouraging,
  title={Encouraging divergent thinking in large language models through multi-agent debate},
  author={Liang, Tian and He, Zhiwei and Jiao, Wenxiang and Wang, Xing and Wang, Yan and Wang, Rui and Yang, Yujiu and Shi, Shuming and Tu, Zhaopeng},
  journal={arXiv preprint arXiv:2305.19118},
  year={2023}
}







