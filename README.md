# Awesome-ICL-and-Reasoning

This repo aims to provides a comprehensive list of papers about LLM inference (algorithm perspective). Specifically, this repo includes ICL, reasoning (single call or multi calls/agents), planning, interact with external tools/memory.


# In-context Learning
Here are all the paper items transformed into the requested format:

What learning algorithm is in-context learning? investigations with linear models[ICLR][2023]

In-context language learning: Architectures and algorithms[arXiv][2024]

Language models are few-shot learners[NeurIPS][2020]

Evaluating large language models trained on code[arXiv][2021]

Why can {GPT} learn in-context? language models secretly perform gradient descent as meta-optimizers[ACL][2023]

Model-agnostic meta-learning for fast adaptation of deep networks[ICML][2017]

What can transformers learn in-context? A case study of simple function classes[NeurIPS][2022]

A theory of emergent in-context learning as implicit structure induction[arXiv][2023]

Mining and summarizing customer reviews[KDD][2004]

ChatGPT for good? On opportunities and challenges of large language models for education[Learning and Individual Differences][2023]

In-context learning learns label relationships but is not conventional learning[ICLR][2024]

WordNet: A lexical database for English[HLT][1994]

Rethinking the role of demonstrations: What makes in-context learning work?[EMNLP][2022]

In-context learning and induction heads[arXiv][2022]

{GPT-4} technical report[arXiv][2023]

What in-context learning "learns" in-context: Disentangling task recognition and task learning[ACL][2023]

Language models are unsupervised multitask learners[GitHub][2019]

Pretraining task diversity and the emergence of non-bayesian in-context learning for regression[NeurIPS][2023]

Evolutionary principles in self-referential learning, or on learning how to learn: The meta-meta-... hook[PhD thesis][1987]

Do pretrained transformers learn in-context by gradient descent?[arXiv][2024]

The transient nature of emergent in-context learning in transformers[NeurIPS][2023]

Recursive deep models for semantic compositionality over a sentiment treebank[EMNLP][2013]

Large language models in medicine[Nature Medicine][2023]

Learning to Learn: Introduction and Overview[Springer][1998]

Llama 2: Open foundation and fine-tuned chat models[arXiv][2023]

A perspective view and survey of meta-learning[Artificial Intelligence Review][2001]

Transformers learn in-context by gradient descent[ICML][2023]

Large language models are implicitly topic models: Explaining and finding good demonstrations for in-context learning[Workshop][2023]

Emergent abilities of large language models[Machine Learning Research][2022]

Larger language models do in-context learning differently[arXiv][2023]

The learnability of in-context learning[NeurIPS][2023]

Transformers: State-of-the-art natural language processing[EMNLP][2020]

An explanation of in-context learning as implicit Bayesian inference[ICLR][2022]

Character-level convolutional networks for text classification[NeurIPS][2015]



# Reasoning Single call/multi calls/agents





# Interact with external tools/memory























